Epoch 0/9
----------
Batch 50 Loss 0.9040
Batch 100 Loss 0.8387
Batch 150 Loss 0.7042
Batch 200 Loss 0.5293
Batch 250 Loss 0.6165
Batch 300 Loss 0.5541
Batch 350 Loss 0.6498
Batch 400 Loss 0.7129
Batch 450 Loss 0.7642
Batch 500 Loss 0.6122
Batch 550 Loss 0.7195
Batch 600 Loss 0.6340
Batch 650 Loss 0.7975
Batch 700 Loss 0.6254
Batch 750 Loss 0.6086
Batch 800 Loss 0.7044
Batch 850 Loss 0.7005
train Loss: 0.7187 Acc: 0.7221 F1: 0.7967 AUC 0.5403
val Loss: 0.9604 Acc: 0.9507 F1: 0.9267 AUC 0.5000
-----
Epoch 1/9
----------
Batch 50 Loss 0.6352
Batch 100 Loss 0.6392
Batch 150 Loss 0.6361
Batch 200 Loss 0.7957
Batch 250 Loss 1.1801
Batch 300 Loss 0.5365
Batch 350 Loss 0.6545
Batch 400 Loss 0.5647
Batch 450 Loss 0.7471
Batch 500 Loss 0.6229
Batch 550 Loss 0.7538
Batch 600 Loss 0.7950
Batch 650 Loss 0.7950
Batch 700 Loss 0.6414
Batch 750 Loss 0.6833
Batch 800 Loss 0.7025
Batch 850 Loss 0.6403
train Loss: 0.6878 Acc: 0.7702 F1: 0.8276 AUC 0.5315
val Loss: 0.6798 Acc: 0.9403 F1: 0.9225 AUC 0.5010
-----
Epoch 2/9
----------
Batch 50 Loss 0.5003
Batch 100 Loss 0.7034
Batch 150 Loss 0.5457
Batch 200 Loss 0.5869
Batch 250 Loss 0.6900
Batch 300 Loss 0.6111
Batch 350 Loss 0.4770
Batch 400 Loss 0.8651
Batch 450 Loss 1.1933
Batch 500 Loss 0.5211
Batch 550 Loss 0.6146
Batch 600 Loss 0.5172
Batch 650 Loss 0.7193
Batch 700 Loss 0.6082
Batch 750 Loss 0.6782
Batch 800 Loss 0.6826
Batch 850 Loss 0.7792
train Loss: 0.6788 Acc: 0.7697 F1: 0.8274 AUC 0.5373
val Loss: 0.6597 Acc: 0.6910 F1: 0.7785 AUC 0.5666
-----
Epoch 3/9
----------
Batch 50 Loss 0.5762
Batch 100 Loss 0.9962
Batch 150 Loss 0.5607
Batch 200 Loss 0.6215
Batch 250 Loss 0.5908
Batch 300 Loss 0.5706
Batch 350 Loss 0.7656
Batch 400 Loss 0.7385
Batch 450 Loss 0.7164
Batch 500 Loss 0.6311
Batch 550 Loss 0.5077
Batch 600 Loss 0.6394
Batch 650 Loss 0.5972
Batch 700 Loss 0.5784
Batch 750 Loss 1.0678
Batch 800 Loss 0.5402
Batch 850 Loss 0.6153
train Loss: 0.6592 Acc: 0.7571 F1: 0.8203 AUC 0.5768
val Loss: 0.6435 Acc: 0.7514 F1: 0.8194 AUC 0.5822
-----
Epoch 4/9
----------
Batch 50 Loss 0.5792
Batch 100 Loss 1.0310
Batch 150 Loss 0.5539
Batch 200 Loss 0.6629
Batch 250 Loss 0.6431
Batch 300 Loss 0.4428
Batch 350 Loss 0.5685
Batch 400 Loss 0.5744
Batch 450 Loss 0.4210
Batch 500 Loss 0.6362
Batch 550 Loss 0.7297
Batch 600 Loss 0.5091
Batch 650 Loss 0.6148
Batch 700 Loss 0.6610
Batch 750 Loss 0.4431
Batch 800 Loss 0.7065
Batch 850 Loss 0.5766
train Loss: 0.6540 Acc: 0.7414 F1: 0.8101 AUC 0.5828
val Loss: 0.6322 Acc: 0.8668 F1: 0.8891 AUC 0.5623
-----
Epoch 5/9
----------
Batch 50 Loss 0.4624
Batch 100 Loss 0.6552
Batch 150 Loss 0.5474
Batch 200 Loss 0.6038
Batch 250 Loss 0.6075
Batch 300 Loss 0.5933
Batch 350 Loss 0.7497
Batch 400 Loss 0.8749
Batch 450 Loss 0.5674
Batch 500 Loss 0.7744
Batch 550 Loss 0.6873
Batch 600 Loss 0.9062
Batch 650 Loss 0.8536
Batch 700 Loss 0.4619
Batch 750 Loss 0.9653
Batch 800 Loss 0.6488
Batch 850 Loss 0.5392
train Loss: 0.6424 Acc: 0.7542 F1: 0.8186 AUC 0.5900
val Loss: 0.6236 Acc: 0.7484 F1: 0.8180 AUC 0.6290
-----
Epoch 6/9
----------
Batch 50 Loss 0.5233
Batch 100 Loss 0.5470
Batch 150 Loss 0.6835
Batch 200 Loss 0.4952
Batch 250 Loss 0.5151
Batch 300 Loss 0.6816
Batch 350 Loss 0.5433
Batch 400 Loss 0.7687
Batch 450 Loss 0.6432
Batch 500 Loss 0.5228
Batch 550 Loss 0.6877
Batch 600 Loss 0.7318
Batch 650 Loss 0.5153
Batch 700 Loss 0.8729
Batch 750 Loss 0.4639
Batch 800 Loss 0.5174
Batch 850 Loss 0.5429
train Loss: 0.6287 Acc: 0.7598 F1: 0.8227 AUC 0.6180
val Loss: 0.6178 Acc: 0.6978 F1: 0.7834 AUC 0.6604
-----
Epoch 7/9
----------
Batch 50 Loss 0.9568
Batch 100 Loss 0.4887
Batch 150 Loss 0.4709
Batch 200 Loss 0.5582
Batch 250 Loss 0.6085
Batch 300 Loss 0.7448
Batch 350 Loss 0.4401
Batch 400 Loss 0.5194
Batch 450 Loss 0.5017
Batch 500 Loss 0.4940
Batch 550 Loss 0.6020
Batch 600 Loss 0.7330
Batch 650 Loss 0.5793
Batch 700 Loss 0.4566
Batch 750 Loss 0.6770
Batch 800 Loss 0.8212
Batch 850 Loss 0.5446
train Loss: 0.5827 Acc: 0.7649 F1: 0.8271 AUC 0.6776
val Loss: 0.6449 Acc: 0.5398 F1: 0.6581 AUC 0.6741
-----
Epoch 8/9
----------
Batch 50 Loss 0.4794
Batch 100 Loss 0.6995
Batch 150 Loss 0.4367
Batch 200 Loss 0.4401
Batch 250 Loss 1.2046
Batch 300 Loss 0.5123
Batch 350 Loss 0.3917
Batch 400 Loss 0.7270
Batch 450 Loss 0.5321
Batch 500 Loss 0.4965
Batch 550 Loss 0.5294
Batch 600 Loss 0.6301
Batch 650 Loss 0.5653
Batch 700 Loss 0.2980
Batch 750 Loss 0.3410
Batch 800 Loss 0.4529
Batch 850 Loss 0.4287
train Loss: 0.5466 Acc: 0.7825 F1: 0.8392 AUC 0.7102
val Loss: 0.5586 Acc: 0.6548 F1: 0.7515 AUC 0.7378
-----
Epoch 9/9
----------
Batch 50 Loss 0.3372
Batch 100 Loss 0.8849
Batch 150 Loss 0.7188
Batch 200 Loss 0.3930
Batch 250 Loss 0.4959
Batch 300 Loss 0.8828
Batch 350 Loss 0.5611
Batch 400 Loss 0.3488
Batch 450 Loss 0.3783
Batch 500 Loss 0.3946
Batch 550 Loss 0.4115
Batch 600 Loss 0.4516
Batch 650 Loss 0.4817
Batch 700 Loss 0.4675
Batch 750 Loss 0.4016
Batch 800 Loss 0.7960
Batch 850 Loss 0.3647
train Loss: 0.4960 Acc: 0.8132 F1: 0.8600 AUC 0.7522
val Loss: 0.4844 Acc: 0.7796 F1: 0.8403 AUC 0.7712
-----
Training complete in 692m 49s
Best Val F1: 0.926655

Evaluating test_kaggle
test_kaggle Loss: 0.6917 Acc: 0.9471 F1: 0.9213
              precision    recall  f1-score   support

           0       0.95      1.00      0.97     42463
           1       0.00      0.00      0.00      2374

   micro avg       0.95      0.95      0.95     44837
   macro avg       0.47      0.50      0.49     44837
weighted avg       0.90      0.95      0.92     44837

[[42463     0]
 [ 2374     0]]
AUC: 0.5000

Evaluating test_wa
test_wa Loss: 0.7960 Acc: 0.0000 F1: 0.0000
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00        45

   micro avg       0.00      0.00      0.00        45
   macro avg       0.00      0.00      0.00        45
weighted avg       0.00      0.00      0.00        45

[[ 0  0]
 [45  0]]
 
 Evaluating test_wa
test_wa Loss: 0.7983 Acc: 0.0000 F1: 0.0000
              precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00        45

   micro avg       0.00      0.00      0.00        45
   macro avg       0.00      0.00      0.00        45
weighted avg       0.00      0.00      0.00        45

[[ 0  0]
 [45  0]]
Evaluating test_deye
test_deye Loss: 0.8158 Acc: 0.4737 F1: 0.3045
              precision    recall  f1-score   support

           0       0.47      1.00      0.64        18
           1       0.00      0.00      0.00        20

   micro avg       0.47      0.47      0.47        38
   macro avg       0.24      0.50      0.32        38
weighted avg       0.22      0.47      0.30        38

[[18  0]
 [20  0]]
-----