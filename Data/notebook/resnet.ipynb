{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27161264"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.inception_v3(pretrained=True)\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 10\n",
    "BATCH_SIZE = 40\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAIN_DATA_PATH = \"train/\"\n",
    "VAL_DATA_PATH = \"val/\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(1, 1.5), saturation=(1, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM)\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "val_data = torchvision.datasets.ImageFolder(root=VAL_DATA_PATH, transform=TRANSFORM)\n",
    "val_data_loader = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = {'train': train_data_loader, 'val': val_data_loader}\n",
    "dataset_sizes = {'train': len(train_data),'val':len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 26430, 'val': 2981}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            batch = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "                \n",
    "                if batch % 50 == 0 and phase == 'train':\n",
    "                    print('Batch {} Loss {:.4f}'.format(batch, loss.item()))\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "            epoch_auc = roc_auc_score(epoch_truth, epoch_pred)\n",
    "                          \n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f} AUC {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1,\n",
    "                                                                            epoch_auc))\n",
    "            \n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('-'*5)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet152(pretrained=True)\n",
    "\n",
    "print(model_ft.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.fc = nn.Linear(2048, NUM_CLASSES)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "w = [0.10598645840316419, 1.894013541596836]\n",
    "\n",
    "weights = torch.tensor(w).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Batch 50 Loss 0.6297\n",
      "Batch 100 Loss 0.8588\n",
      "Batch 150 Loss 0.7466\n",
      "Batch 200 Loss 0.7370\n",
      "Batch 250 Loss 0.6507\n",
      "Batch 300 Loss 0.6954\n",
      "Batch 350 Loss 0.8079\n",
      "Batch 400 Loss 0.7244\n",
      "Batch 450 Loss 0.6518\n",
      "Batch 500 Loss 0.6143\n",
      "Batch 550 Loss 0.6160\n",
      "Batch 600 Loss 0.6834\n",
      "Batch 650 Loss 0.9159\n",
      "train Loss: 0.7235 Acc: 0.7861 F1: 0.8371 AUC 0.5212\n",
      "val Loss: 0.8381 Acc: 0.7407 F1: 0.8114 AUC 0.5153\n",
      "-----\n",
      "Epoch 1/9\n",
      "----------\n",
      "Batch 50 Loss 0.9443\n",
      "Batch 100 Loss 0.6964\n",
      "Batch 150 Loss 0.5843\n",
      "Batch 200 Loss 0.6853\n",
      "Batch 250 Loss 0.6981\n",
      "Batch 300 Loss 0.7047\n",
      "Batch 350 Loss 0.7013\n",
      "Batch 400 Loss 0.8110\n",
      "Batch 450 Loss 0.7085\n",
      "Batch 500 Loss 0.4924\n",
      "Batch 550 Loss 0.3834\n",
      "Batch 600 Loss 0.6022\n",
      "Batch 650 Loss 0.4539\n",
      "train Loss: 0.6393 Acc: 0.7742 F1: 0.8320 AUC 0.6172\n",
      "val Loss: 0.6245 Acc: 0.9336 F1: 0.9372 AUC 0.7071\n",
      "-----\n",
      "Epoch 2/9\n",
      "----------\n",
      "Batch 50 Loss 0.5396\n",
      "Batch 100 Loss 0.2822\n",
      "Batch 150 Loss 0.4246\n",
      "Batch 200 Loss 0.1651\n",
      "Batch 250 Loss 0.3468\n",
      "Batch 300 Loss 0.1560\n",
      "Batch 350 Loss 0.7544\n",
      "Batch 400 Loss 0.1619\n",
      "Batch 450 Loss 0.3432\n",
      "Batch 500 Loss 0.2293\n",
      "Batch 550 Loss 0.1611\n",
      "Batch 600 Loss 0.2515\n",
      "Batch 650 Loss 0.6069\n",
      "train Loss: 0.4059 Acc: 0.8764 F1: 0.9025 AUC 0.8163\n",
      "val Loss: 0.5014 Acc: 0.9332 F1: 0.9374 AUC 0.7166\n",
      "-----\n",
      "Epoch 3/9\n",
      "----------\n",
      "Batch 50 Loss 0.2147\n",
      "Batch 100 Loss 0.3367\n",
      "Batch 150 Loss 0.1057\n",
      "Batch 200 Loss 0.5542\n",
      "Batch 250 Loss 0.3639\n",
      "Batch 300 Loss 0.2577\n",
      "Batch 350 Loss 0.3257\n",
      "Batch 400 Loss 0.1455\n",
      "Batch 450 Loss 0.1396\n",
      "Batch 500 Loss 0.3213\n",
      "Batch 550 Loss 0.1579\n",
      "Batch 600 Loss 0.3249\n",
      "Batch 650 Loss 0.3136\n",
      "train Loss: 0.3057 Acc: 0.9052 F1: 0.9231 AUC 0.8750\n",
      "val Loss: 0.3564 Acc: 0.8856 F1: 0.9110 AUC 0.8592\n",
      "-----\n",
      "Epoch 4/9\n",
      "----------\n",
      "Batch 50 Loss 0.3132\n",
      "Batch 100 Loss 0.1518\n",
      "Batch 150 Loss 0.0926\n",
      "Batch 200 Loss 0.4152\n",
      "Batch 250 Loss 0.1533\n",
      "Batch 300 Loss 0.4127\n",
      "Batch 350 Loss 0.2062\n",
      "Batch 400 Loss 0.1518\n",
      "Batch 450 Loss 0.0977\n",
      "Batch 500 Loss 0.0951\n",
      "Batch 550 Loss 0.0889\n",
      "Batch 600 Loss 0.2100\n",
      "Batch 650 Loss 0.1510\n",
      "train Loss: 0.2586 Acc: 0.9205 F1: 0.9342 AUC 0.8965\n",
      "val Loss: 0.3572 Acc: 0.8759 F1: 0.9047 AUC 0.8638\n",
      "-----\n",
      "Epoch 5/9\n",
      "----------\n",
      "Batch 50 Loss 0.2087\n",
      "Batch 100 Loss 0.6276\n",
      "Batch 150 Loss 0.4809\n",
      "Batch 200 Loss 0.2993\n",
      "Batch 250 Loss 0.2255\n",
      "Batch 300 Loss 0.3705\n",
      "Batch 350 Loss 0.1165\n",
      "Batch 400 Loss 0.3784\n",
      "Batch 450 Loss 0.1036\n",
      "Batch 500 Loss 0.2852\n",
      "Batch 550 Loss 0.0808\n",
      "Batch 600 Loss 0.1660\n",
      "Batch 650 Loss 0.2512\n",
      "train Loss: 0.2144 Acc: 0.9305 F1: 0.9417 AUC 0.9135\n",
      "val Loss: 0.3534 Acc: 0.9319 F1: 0.9418 AUC 0.8545\n",
      "-----\n",
      "Epoch 6/9\n",
      "----------\n",
      "Batch 50 Loss 0.0376\n",
      "Batch 100 Loss 0.3309\n",
      "Batch 150 Loss 0.1182\n",
      "Batch 200 Loss 0.2913\n",
      "Batch 250 Loss 0.2196\n",
      "Batch 300 Loss 0.1263\n",
      "Batch 350 Loss 0.0617\n",
      "Batch 400 Loss 0.0546\n",
      "Batch 450 Loss 0.3226\n",
      "Batch 500 Loss 0.0677\n",
      "Batch 550 Loss 0.0398\n",
      "Batch 600 Loss 0.0933\n",
      "Batch 650 Loss 0.0327\n",
      "train Loss: 0.1854 Acc: 0.9364 F1: 0.9461 AUC 0.9246\n",
      "val Loss: 0.3338 Acc: 0.8615 F1: 0.8953 AUC 0.8626\n",
      "-----\n",
      "Epoch 7/9\n",
      "----------\n",
      "Batch 50 Loss 0.3035\n",
      "Batch 100 Loss 0.0505\n",
      "Batch 150 Loss 0.1657\n",
      "Batch 200 Loss 0.1497\n",
      "Batch 250 Loss 0.0488\n",
      "Batch 300 Loss 0.0431\n",
      "Batch 350 Loss 0.2852\n",
      "Batch 400 Loss 0.0415\n",
      "Batch 450 Loss 0.0551\n",
      "Batch 500 Loss 0.1045\n",
      "Batch 550 Loss 0.2625\n",
      "Batch 600 Loss 0.0344\n",
      "Batch 650 Loss 0.2190\n",
      "train Loss: 0.1375 Acc: 0.9489 F1: 0.9559 AUC 0.9473\n",
      "val Loss: 0.3313 Acc: 0.9285 F1: 0.9403 AUC 0.8850\n",
      "-----\n",
      "Epoch 8/9\n",
      "----------\n",
      "Batch 50 Loss 0.0766\n",
      "Batch 100 Loss 0.1918\n",
      "Batch 150 Loss 0.0715\n",
      "Batch 200 Loss 0.0439\n",
      "Batch 250 Loss 0.3616\n",
      "Batch 300 Loss 0.0375\n",
      "Batch 350 Loss 0.0438\n",
      "Batch 400 Loss 0.0655\n",
      "Batch 450 Loss 0.0725\n",
      "Batch 500 Loss 0.0570\n",
      "Batch 550 Loss 0.0625\n",
      "Batch 600 Loss 0.0945\n",
      "Batch 650 Loss 0.0353\n",
      "train Loss: 0.1134 Acc: 0.9577 F1: 0.9628 AUC 0.9586\n",
      "val Loss: 0.3671 Acc: 0.9302 F1: 0.9411 AUC 0.8698\n",
      "-----\n",
      "Epoch 9/9\n",
      "----------\n",
      "Batch 50 Loss 0.1105\n",
      "Batch 100 Loss 0.0284\n",
      "Batch 150 Loss 0.0639\n",
      "Batch 200 Loss 0.0528\n",
      "Batch 250 Loss 0.0462\n",
      "Batch 300 Loss 0.0804\n",
      "Batch 350 Loss 0.1948\n",
      "Batch 400 Loss 0.0227\n",
      "Batch 450 Loss 0.0674\n",
      "Batch 500 Loss 0.0767\n",
      "Batch 550 Loss 0.2169\n",
      "Batch 600 Loss 0.0983\n",
      "Batch 650 Loss 0.0705\n",
      "train Loss: 0.0881 Acc: 0.9641 F1: 0.9680 AUC 0.9683\n",
      "val Loss: 0.4926 Acc: 0.9477 F1: 0.9536 AUC 0.8725\n",
      "-----\n",
      "Training complete in 646m 27s\n",
      "Best Val F1: 0.953566\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'nova_res.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"test/\"\n",
    "WA_DATA_PATH = \"test_wa/\"\n",
    "DEYE_DATA_PATH = \"test_deye/\"\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "wa_data = torchvision.datasets.ImageFolder(root=WA_DATA_PATH, transform=TRANSFORM)\n",
    "wa_data_loader = data.DataLoader(wa_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "deye_data = torchvision.datasets.ImageFolder(root=DEYE_DATA_PATH, transform=TRANSFORM)\n",
    "deye_data_loader = data.DataLoader(deye_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "\n",
    "dataloaders = {'test_kaggle': test_data_loader, 'test_wa': wa_data_loader, 'test_deye': deye_data_loader}\n",
    "dataset_sizes = {'test_kaggle': len(test_data),'test_wa':len(wa_data), 'test_deye':len(deye_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_kaggle': 44836, 'test_wa': 45, 'test_deye': 38}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def my_plot(truth, pred):\n",
    "    auc = roc_auc_score(truth, pred)\n",
    "    print('AUC: %.4f' % auc)\n",
    "    return\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('Specificity / True Negative Rate')\n",
    "    plt.ylabel('Sensitivity / Recall')\n",
    "    plt.savefig(str(cnt) + \".jpg\")\n",
    "    cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion, optimizer, scheduler):\n",
    "    model.load_state_dict(torch.load('nova_res.pth'))\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(1):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        # for phase in ['test_kaggle', 'test_wa', 'test_deye']:\n",
    "        for phase in ['test_wa', 'test_deye']:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "            print('Evaluating {}'.format(phase))\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            batch = 0\n",
    "            # batch_start = time.time()\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "                          \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1))\n",
    "            print(classification_report(epoch_truth, epoch_pred))\n",
    "            print(confusion_matrix(epoch_truth, epoch_pred))\n",
    "            # my_plot(epoch_truth, epoch_pred)\n",
    "            \n",
    "        print('-'*5)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_wa\n",
      "test_wa Loss: 6.5761 Acc: 0.2667 F1: 0.4211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.27      0.42        45\n",
      "\n",
      "   micro avg       0.27      0.27      0.27        45\n",
      "   macro avg       0.50      0.13      0.21        45\n",
      "weighted avg       1.00      0.27      0.42        45\n",
      "\n",
      "[[ 0  0]\n",
      " [33 12]]\n",
      "Evaluating test_deye\n",
      "test_deye Loss: 0.2875 Acc: 0.5263 F1: 0.3630\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.53      1.00      0.69        20\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        38\n",
      "   macro avg       0.26      0.50      0.34        38\n",
      "weighted avg       0.28      0.53      0.36        38\n",
      "\n",
      "[[ 0 18]\n",
      " [ 0 20]]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "eval_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
