{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 10\n",
    "BATCH_SIZE = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAIN_DATA_PATH = \"train/\"\n",
    "VAL_DATA_PATH = \"val/\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(1, 1.5), saturation=(1, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM)\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "val_data = torchvision.datasets.ImageFolder(root=VAL_DATA_PATH, transform=TRANSFORM)\n",
    "val_data_loader = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = {'train': train_data_loader, 'val': val_data_loader}\n",
    "dataset_sizes = {'train': len(train_data),'val':len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 26430, 'val': 2981}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            batch = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "                \n",
    "                if batch % 50 == 0 and phase == 'train':\n",
    "                    print('Batch {} Loss {:.4f}'.format(batch, loss.item()))\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "            epoch_auc = roc_auc_score(epoch_truth, epoch_pred)\n",
    "                          \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f} AUC {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1,\n",
    "                                                                            epoch_auc))\n",
    "            \n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('-'*5)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2208, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.densenet161(pretrained=True)\n",
    "\n",
    "print(model_ft.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.classifier = nn.Linear(2208, NUM_CLASSES)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "w = [0.10598645840316419, 1.894013541596836]\n",
    "\n",
    "weights = torch.tensor(w).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Batch 50 Loss 0.9040\n",
      "Batch 100 Loss 0.8387\n",
      "Batch 150 Loss 0.7042\n",
      "Batch 200 Loss 0.5293\n",
      "Batch 250 Loss 0.6165\n",
      "Batch 300 Loss 0.5541\n",
      "Batch 350 Loss 0.6498\n",
      "Batch 400 Loss 0.7129\n",
      "Batch 450 Loss 0.7642\n",
      "Batch 500 Loss 0.6122\n",
      "Batch 550 Loss 0.7195\n",
      "Batch 600 Loss 0.6340\n",
      "Batch 650 Loss 0.7975\n",
      "Batch 700 Loss 0.6254\n",
      "Batch 750 Loss 0.6086\n",
      "Batch 800 Loss 0.7044\n",
      "Batch 850 Loss 0.7005\n",
      "train Loss: 0.7187 Acc: 0.7221 F1: 0.7967 AUC 0.5403\n",
      "val Loss: 0.9604 Acc: 0.9507 F1: 0.9267 AUC 0.5000\n",
      "-----\n",
      "Epoch 1/9\n",
      "----------\n",
      "Batch 50 Loss 0.6352\n",
      "Batch 100 Loss 0.6392\n",
      "Batch 150 Loss 0.6361\n",
      "Batch 200 Loss 0.7957\n",
      "Batch 250 Loss 1.1801\n",
      "Batch 300 Loss 0.5365\n",
      "Batch 350 Loss 0.6545\n",
      "Batch 400 Loss 0.5647\n",
      "Batch 450 Loss 0.7471\n",
      "Batch 500 Loss 0.6229\n",
      "Batch 550 Loss 0.7538\n",
      "Batch 600 Loss 0.7950\n",
      "Batch 650 Loss 0.7950\n",
      "Batch 700 Loss 0.6414\n",
      "Batch 750 Loss 0.6833\n",
      "Batch 800 Loss 0.7025\n",
      "Batch 850 Loss 0.6403\n",
      "train Loss: 0.6878 Acc: 0.7702 F1: 0.8276 AUC 0.5315\n",
      "val Loss: 0.6798 Acc: 0.9403 F1: 0.9225 AUC 0.5010\n",
      "-----\n",
      "Epoch 2/9\n",
      "----------\n",
      "Batch 50 Loss 0.5003\n",
      "Batch 100 Loss 0.7034\n",
      "Batch 150 Loss 0.5457\n",
      "Batch 200 Loss 0.5869\n",
      "Batch 250 Loss 0.6900\n",
      "Batch 300 Loss 0.6111\n",
      "Batch 350 Loss 0.4770\n",
      "Batch 400 Loss 0.8651\n",
      "Batch 450 Loss 1.1933\n",
      "Batch 500 Loss 0.5211\n",
      "Batch 550 Loss 0.6146\n",
      "Batch 600 Loss 0.5172\n",
      "Batch 650 Loss 0.7193\n",
      "Batch 700 Loss 0.6082\n",
      "Batch 750 Loss 0.6782\n",
      "Batch 800 Loss 0.6826\n",
      "Batch 850 Loss 0.7792\n",
      "train Loss: 0.6788 Acc: 0.7697 F1: 0.8274 AUC 0.5373\n",
      "val Loss: 0.6597 Acc: 0.6910 F1: 0.7785 AUC 0.5666\n",
      "-----\n",
      "Epoch 3/9\n",
      "----------\n",
      "Batch 50 Loss 0.5762\n",
      "Batch 100 Loss 0.9962\n",
      "Batch 150 Loss 0.5607\n",
      "Batch 200 Loss 0.6215\n",
      "Batch 250 Loss 0.5908\n",
      "Batch 300 Loss 0.5706\n",
      "Batch 350 Loss 0.7656\n",
      "Batch 400 Loss 0.7385\n",
      "Batch 450 Loss 0.7164\n",
      "Batch 500 Loss 0.6311\n",
      "Batch 550 Loss 0.5077\n",
      "Batch 600 Loss 0.6394\n",
      "Batch 650 Loss 0.5972\n",
      "Batch 700 Loss 0.5784\n",
      "Batch 750 Loss 1.0678\n",
      "Batch 800 Loss 0.5402\n",
      "Batch 850 Loss 0.6153\n",
      "train Loss: 0.6592 Acc: 0.7571 F1: 0.8203 AUC 0.5768\n",
      "val Loss: 0.6435 Acc: 0.7514 F1: 0.8194 AUC 0.5822\n",
      "-----\n",
      "Epoch 4/9\n",
      "----------\n",
      "Batch 50 Loss 0.5792\n",
      "Batch 100 Loss 1.0310\n",
      "Batch 150 Loss 0.5539\n",
      "Batch 200 Loss 0.6629\n",
      "Batch 250 Loss 0.6431\n",
      "Batch 300 Loss 0.4428\n",
      "Batch 350 Loss 0.5685\n",
      "Batch 400 Loss 0.5744\n",
      "Batch 450 Loss 0.4210\n",
      "Batch 500 Loss 0.6362\n",
      "Batch 550 Loss 0.7297\n",
      "Batch 600 Loss 0.5091\n",
      "Batch 650 Loss 0.6148\n",
      "Batch 700 Loss 0.6610\n",
      "Batch 750 Loss 0.4431\n",
      "Batch 800 Loss 0.7065\n",
      "Batch 850 Loss 0.5766\n",
      "train Loss: 0.6540 Acc: 0.7414 F1: 0.8101 AUC 0.5828\n",
      "val Loss: 0.6322 Acc: 0.8668 F1: 0.8891 AUC 0.5623\n",
      "-----\n",
      "Epoch 5/9\n",
      "----------\n",
      "Batch 50 Loss 0.4624\n",
      "Batch 100 Loss 0.6552\n",
      "Batch 150 Loss 0.5474\n",
      "Batch 200 Loss 0.6038\n",
      "Batch 250 Loss 0.6075\n",
      "Batch 300 Loss 0.5933\n",
      "Batch 350 Loss 0.7497\n",
      "Batch 400 Loss 0.8749\n",
      "Batch 450 Loss 0.5674\n",
      "Batch 500 Loss 0.7744\n",
      "Batch 550 Loss 0.6873\n",
      "Batch 600 Loss 0.9062\n",
      "Batch 650 Loss 0.8536\n",
      "Batch 700 Loss 0.4619\n",
      "Batch 750 Loss 0.9653\n",
      "Batch 800 Loss 0.6488\n",
      "Batch 850 Loss 0.5392\n",
      "train Loss: 0.6424 Acc: 0.7542 F1: 0.8186 AUC 0.5900\n",
      "val Loss: 0.6236 Acc: 0.7484 F1: 0.8180 AUC 0.6290\n",
      "-----\n",
      "Epoch 6/9\n",
      "----------\n",
      "Batch 50 Loss 0.5233\n",
      "Batch 100 Loss 0.5470\n",
      "Batch 150 Loss 0.6835\n",
      "Batch 200 Loss 0.4952\n",
      "Batch 250 Loss 0.5151\n",
      "Batch 300 Loss 0.6816\n",
      "Batch 350 Loss 0.5433\n",
      "Batch 400 Loss 0.7687\n",
      "Batch 450 Loss 0.6432\n",
      "Batch 500 Loss 0.5228\n",
      "Batch 550 Loss 0.6877\n",
      "Batch 600 Loss 0.7318\n",
      "Batch 650 Loss 0.5153\n",
      "Batch 700 Loss 0.8729\n",
      "Batch 750 Loss 0.4639\n",
      "Batch 800 Loss 0.5174\n",
      "Batch 850 Loss 0.5429\n",
      "train Loss: 0.6287 Acc: 0.7598 F1: 0.8227 AUC 0.6180\n",
      "val Loss: 0.6178 Acc: 0.6978 F1: 0.7834 AUC 0.6604\n",
      "-----\n",
      "Epoch 7/9\n",
      "----------\n",
      "Batch 50 Loss 0.9568\n",
      "Batch 100 Loss 0.4887\n",
      "Batch 150 Loss 0.4709\n",
      "Batch 200 Loss 0.5582\n",
      "Batch 250 Loss 0.6085\n",
      "Batch 300 Loss 0.7448\n",
      "Batch 350 Loss 0.4401\n",
      "Batch 400 Loss 0.5194\n",
      "Batch 450 Loss 0.5017\n",
      "Batch 500 Loss 0.4940\n",
      "Batch 550 Loss 0.6020\n",
      "Batch 600 Loss 0.7330\n",
      "Batch 650 Loss 0.5793\n",
      "Batch 700 Loss 0.4566\n",
      "Batch 750 Loss 0.6770\n",
      "Batch 800 Loss 0.8212\n",
      "Batch 850 Loss 0.5446\n",
      "train Loss: 0.5827 Acc: 0.7649 F1: 0.8271 AUC 0.6776\n",
      "val Loss: 0.6449 Acc: 0.5398 F1: 0.6581 AUC 0.6741\n",
      "-----\n",
      "Epoch 8/9\n",
      "----------\n",
      "Batch 50 Loss 0.4794\n",
      "Batch 100 Loss 0.6995\n",
      "Batch 150 Loss 0.4367\n",
      "Batch 200 Loss 0.4401\n",
      "Batch 250 Loss 1.2046\n",
      "Batch 300 Loss 0.5123\n",
      "Batch 350 Loss 0.3917\n",
      "Batch 400 Loss 0.7270\n",
      "Batch 450 Loss 0.5321\n",
      "Batch 500 Loss 0.4965\n",
      "Batch 550 Loss 0.5294\n",
      "Batch 600 Loss 0.6301\n",
      "Batch 650 Loss 0.5653\n",
      "Batch 700 Loss 0.2980\n",
      "Batch 750 Loss 0.3410\n",
      "Batch 800 Loss 0.4529\n",
      "Batch 850 Loss 0.4287\n",
      "train Loss: 0.5466 Acc: 0.7825 F1: 0.8392 AUC 0.7102\n",
      "val Loss: 0.5586 Acc: 0.6548 F1: 0.7515 AUC 0.7378\n",
      "-----\n",
      "Epoch 9/9\n",
      "----------\n",
      "Batch 50 Loss 0.3372\n",
      "Batch 100 Loss 0.8849\n",
      "Batch 150 Loss 0.7188\n",
      "Batch 200 Loss 0.3930\n",
      "Batch 250 Loss 0.4959\n",
      "Batch 300 Loss 0.8828\n",
      "Batch 350 Loss 0.5611\n",
      "Batch 400 Loss 0.3488\n",
      "Batch 450 Loss 0.3783\n",
      "Batch 500 Loss 0.3946\n",
      "Batch 550 Loss 0.4115\n",
      "Batch 600 Loss 0.4516\n",
      "Batch 650 Loss 0.4817\n",
      "Batch 700 Loss 0.4675\n",
      "Batch 750 Loss 0.4016\n",
      "Batch 800 Loss 0.7960\n",
      "Batch 850 Loss 0.3647\n",
      "train Loss: 0.4960 Acc: 0.8132 F1: 0.8600 AUC 0.7522\n",
      "val Loss: 0.4844 Acc: 0.7796 F1: 0.8403 AUC 0.7712\n",
      "-----\n",
      "Training complete in 692m 49s\n",
      "Best Val F1: 0.926655\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"test/\"\n",
    "WA_DATA_PATH = \"test_wa/\"\n",
    "DEYE_DATA_PATH = \"test_deye/\"\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "wa_data = torchvision.datasets.ImageFolder(root=WA_DATA_PATH, transform=TRANSFORM)\n",
    "wa_data_loader = data.DataLoader(wa_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "deye_data = torchvision.datasets.ImageFolder(root=DEYE_DATA_PATH, transform=TRANSFORM)\n",
    "deye_data_loader = data.DataLoader(deye_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "\n",
    "dataloaders = {'test_kaggle': test_data_loader, 'test_wa': wa_data_loader, 'test_deye': deye_data_loader}\n",
    "dataset_sizes = {'test_kaggle': len(test_data),'test_wa':len(wa_data), 'test_deye':len(deye_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_kaggle': 44837, 'test_wa': 45, 'test_deye': 38}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def my_plot(truth, pred):\n",
    "    auc = roc_auc_score(truth, pred)\n",
    "    print('AUC: %.4f' % auc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion, optimizer, scheduler):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(1):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        # for phase in ['test_kaggle', 'test_wa', 'test_deye']:\n",
    "        for phase in ['test_wa', 'test_deye']:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "            print('Evaluating {}'.format(phase))\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            batch = 0\n",
    "            # batch_start = time.time()\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "                          \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1))\n",
    "            print(classification_report(epoch_truth, epoch_pred))\n",
    "            print(confusion_matrix(epoch_truth, epoch_pred))\n",
    "            # my_plot(epoch_truth, epoch_pred)\n",
    "            \n",
    "        print('-'*5)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_wa\n",
      "test_wa Loss: 0.7983 Acc: 0.0000 F1: 0.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00        45\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        45\n",
      "   macro avg       0.00      0.00      0.00        45\n",
      "weighted avg       0.00      0.00      0.00        45\n",
      "\n",
      "[[ 0  0]\n",
      " [45  0]]\n",
      "Evaluating test_deye\n",
      "test_deye Loss: 0.8158 Acc: 0.4737 F1: 0.3045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        18\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "   micro avg       0.47      0.47      0.47        38\n",
      "   macro avg       0.24      0.50      0.32        38\n",
      "weighted avg       0.22      0.47      0.30        38\n",
      "\n",
      "[[18  0]\n",
      " [20  0]]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "eval_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'nova_dense.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
