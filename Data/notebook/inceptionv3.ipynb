{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER = 10\n",
    "BATCH_SIZE = 40\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAIN_DATA_PATH = \"train/\"\n",
    "VAL_DATA_PATH = \"val/\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(1, 1.5), saturation=(1, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM)\n",
    "train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "val_data = torchvision.datasets.ImageFolder(root=VAL_DATA_PATH, transform=TRANSFORM)\n",
    "val_data_loader = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = {'train': train_data_loader, 'val': val_data_loader}\n",
    "dataset_sizes = {'train': len(train_data),'val':len(val_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 26430, 'val': 2981}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            batch = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # print(inputs.shape)\n",
    "                # break\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "                \n",
    "                if batch % 50 == 0 and phase == 'train':\n",
    "                    print('Batch {} Loss {:.4f}'.format(batch, loss.item()))\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "            epoch_auc = roc_auc_score(epoch_truth, epoch_pred)\n",
    "                          \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f} AUC {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1,\n",
    "                                                                            epoch_auc))\n",
    "            \n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_f1 = epoch_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print('-'*5)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val F1: {:4f}'.format(best_f1))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.inception_v3(pretrained=True)\n",
    "\n",
    "# print(model_ft)\n",
    "print(model_ft.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "model_ft.AuxLogits.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs,NUM_CLASSES)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "w = [0.10598645840316419, 1.894013541596836]\n",
    "\n",
    "weights = torch.tensor(w).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Batch 50 Loss 0.8409\n",
      "Batch 100 Loss 0.7189\n",
      "Batch 150 Loss 0.7586\n",
      "Batch 200 Loss 0.8850\n",
      "Batch 250 Loss 0.9920\n",
      "Batch 300 Loss 0.6323\n",
      "Batch 350 Loss 0.5242\n",
      "Batch 400 Loss 0.3079\n",
      "Batch 450 Loss 0.6452\n",
      "Batch 500 Loss 0.2865\n",
      "Batch 550 Loss 0.6909\n",
      "Batch 600 Loss 0.5049\n",
      "Batch 650 Loss 1.4200\n",
      "train Loss: 0.6437 Acc: 0.8773 F1: 0.9027 AUC 0.8018\n",
      "val Loss: 0.4302 Acc: 0.9554 F1: 0.9570 AUC 0.8024\n",
      "-----\n",
      "Epoch 1/9\n",
      "----------\n",
      "Batch 50 Loss 0.6574\n",
      "Batch 100 Loss 0.2953\n",
      "Batch 150 Loss 0.6869\n",
      "Batch 200 Loss 0.1758\n",
      "Batch 250 Loss 0.2264\n",
      "Batch 300 Loss 0.5584\n",
      "Batch 350 Loss 0.2949\n",
      "Batch 400 Loss 0.3762\n",
      "Batch 450 Loss 0.1947\n",
      "Batch 500 Loss 0.9833\n",
      "Batch 550 Loss 0.4043\n",
      "Batch 600 Loss 0.3893\n",
      "Batch 650 Loss 0.1578\n",
      "train Loss: 0.4014 Acc: 0.9265 F1: 0.9381 AUC 0.8879\n",
      "val Loss: 0.3277 Acc: 0.9413 F1: 0.9487 AUC 0.8627\n",
      "-----\n",
      "Epoch 2/9\n",
      "----------\n",
      "Batch 50 Loss 0.1060\n",
      "Batch 100 Loss 0.9383\n",
      "Batch 150 Loss 0.1540\n",
      "Batch 200 Loss 0.2336\n",
      "Batch 250 Loss 0.0623\n",
      "Batch 300 Loss 0.3083\n",
      "Batch 350 Loss 0.2588\n",
      "Batch 400 Loss 0.9789\n",
      "Batch 450 Loss 0.1980\n",
      "Batch 500 Loss 0.2964\n",
      "Batch 550 Loss 0.2477\n",
      "Batch 600 Loss 0.1451\n",
      "Batch 650 Loss 0.6029\n",
      "train Loss: 0.3338 Acc: 0.9334 F1: 0.9436 AUC 0.9060\n",
      "val Loss: 0.2510 Acc: 0.9202 F1: 0.9346 AUC 0.8871\n",
      "-----\n",
      "Epoch 3/9\n",
      "----------\n",
      "Batch 50 Loss 0.0969\n",
      "Batch 100 Loss 0.0762\n",
      "Batch 150 Loss 0.0652\n",
      "Batch 200 Loss 0.1411\n",
      "Batch 250 Loss 0.0687\n",
      "Batch 300 Loss 0.6228\n",
      "Batch 350 Loss 0.1935\n",
      "Batch 400 Loss 0.1072\n",
      "Batch 450 Loss 0.1586\n",
      "Batch 500 Loss 0.0850\n",
      "Batch 550 Loss 0.0476\n",
      "Batch 600 Loss 0.1341\n",
      "Batch 650 Loss 0.0703\n",
      "train Loss: 0.2381 Acc: 0.9525 F1: 0.9583 AUC 0.9348\n",
      "val Loss: 0.2565 Acc: 0.9403 F1: 0.9489 AUC 0.8976\n",
      "-----\n",
      "Epoch 4/9\n",
      "----------\n",
      "Batch 50 Loss 0.0167\n",
      "Batch 100 Loss 0.0840\n",
      "Batch 150 Loss 0.2435\n",
      "Batch 200 Loss 0.1446\n",
      "Batch 250 Loss 0.2955\n",
      "Batch 300 Loss 0.0319\n",
      "Batch 350 Loss 0.2969\n",
      "Batch 400 Loss 0.0337\n",
      "Batch 450 Loss 0.1655\n",
      "Batch 500 Loss 0.0158\n",
      "Batch 550 Loss 0.1487\n",
      "Batch 600 Loss 0.1548\n",
      "Batch 650 Loss 0.0720\n",
      "train Loss: 0.1864 Acc: 0.9610 F1: 0.9652 AUC 0.9507\n",
      "val Loss: 0.2891 Acc: 0.9108 F1: 0.9287 AUC 0.9047\n",
      "-----\n",
      "Epoch 5/9\n",
      "----------\n",
      "Batch 50 Loss 0.1448\n",
      "Batch 100 Loss 0.0599\n",
      "Batch 150 Loss 0.1002\n",
      "Batch 200 Loss 0.1130\n",
      "Batch 250 Loss 0.0463\n",
      "Batch 300 Loss 0.0585\n",
      "Batch 350 Loss 0.0840\n",
      "Batch 400 Loss 0.0308\n",
      "Batch 450 Loss 0.1418\n",
      "Batch 500 Loss 0.0140\n",
      "Batch 550 Loss 0.0634\n",
      "Batch 600 Loss 0.1015\n",
      "Batch 650 Loss 0.0779\n",
      "train Loss: 0.1316 Acc: 0.9693 F1: 0.9721 AUC 0.9654\n",
      "val Loss: 0.2634 Acc: 0.9487 F1: 0.9554 AUC 0.9117\n",
      "-----\n",
      "Epoch 6/9\n",
      "----------\n",
      "Batch 50 Loss 0.0374\n",
      "Batch 100 Loss 0.0077\n",
      "Batch 150 Loss 0.1950\n",
      "Batch 200 Loss 0.0642\n",
      "Batch 250 Loss 0.0548\n",
      "Batch 300 Loss 0.0337\n",
      "Batch 350 Loss 0.0431\n",
      "Batch 400 Loss 0.1473\n",
      "Batch 450 Loss 0.0081\n",
      "Batch 500 Loss 0.1497\n",
      "Batch 550 Loss 0.0096\n",
      "Batch 600 Loss 0.2759\n",
      "Batch 650 Loss 0.0564\n",
      "train Loss: 0.1090 Acc: 0.9737 F1: 0.9759 AUC 0.9711\n",
      "val Loss: 0.3382 Acc: 0.9423 F1: 0.9504 AUC 0.8987\n",
      "-----\n",
      "Epoch 7/9\n",
      "----------\n",
      "Batch 50 Loss 0.1108\n",
      "Batch 100 Loss 0.0513\n",
      "Batch 150 Loss 0.0194\n",
      "Batch 200 Loss 0.0422\n",
      "Batch 250 Loss 0.0135\n",
      "Batch 300 Loss 0.0764\n",
      "Batch 350 Loss 0.0110\n",
      "Batch 400 Loss 0.3019\n",
      "Batch 450 Loss 0.0894\n",
      "Batch 500 Loss 0.3508\n",
      "Batch 550 Loss 0.1242\n",
      "Batch 600 Loss 0.1268\n",
      "Batch 650 Loss 0.0532\n",
      "train Loss: 0.0784 Acc: 0.9807 F1: 0.9819 AUC 0.9798\n",
      "val Loss: 0.3328 Acc: 0.9644 F1: 0.9671 AUC 0.9007\n",
      "-----\n",
      "Epoch 8/9\n",
      "----------\n",
      "Batch 50 Loss 0.0288\n",
      "Batch 100 Loss 0.0168\n",
      "Batch 150 Loss 0.1070\n",
      "Batch 200 Loss 0.0360\n",
      "Batch 250 Loss 0.0346\n",
      "Batch 300 Loss 0.0318\n",
      "Batch 350 Loss 0.3735\n",
      "Batch 400 Loss 0.0305\n",
      "Batch 450 Loss 0.1271\n",
      "Batch 500 Loss 0.0067\n",
      "Batch 550 Loss 0.0655\n",
      "Batch 600 Loss 0.0747\n",
      "Batch 650 Loss 0.0081\n",
      "train Loss: 0.0653 Acc: 0.9848 F1: 0.9856 AUC 0.9846\n",
      "val Loss: 0.3200 Acc: 0.9628 F1: 0.9660 AUC 0.9095\n",
      "-----\n",
      "Epoch 9/9\n",
      "----------\n",
      "Batch 50 Loss 0.0040\n",
      "Batch 100 Loss 0.0803\n",
      "Batch 150 Loss 0.0793\n",
      "Batch 200 Loss 0.0552\n",
      "Batch 250 Loss 0.0111\n",
      "Batch 300 Loss 0.0327\n",
      "Batch 350 Loss 0.0180\n",
      "Batch 400 Loss 0.0772\n",
      "Batch 450 Loss 0.0361\n",
      "Batch 500 Loss 0.0835\n",
      "Batch 550 Loss 0.0261\n",
      "Batch 600 Loss 0.0021\n",
      "Batch 650 Loss 0.0358\n",
      "train Loss: 0.0561 Acc: 0.9869 F1: 0.9875 AUC 0.9867\n",
      "val Loss: 0.4133 Acc: 0.9665 F1: 0.9685 AUC 0.8921\n",
      "-----\n",
      "Training complete in 635m 54s\n",
      "Best Val F1: 0.968506\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'nova_ins.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"test/\"\n",
    "WA_DATA_PATH = \"test_wa/\"\n",
    "DEYE_DATA_PATH = \"test_deye/\"\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "wa_data = torchvision.datasets.ImageFolder(root=WA_DATA_PATH, transform=TRANSFORM)\n",
    "wa_data_loader = data.DataLoader(wa_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "deye_data = torchvision.datasets.ImageFolder(root=DEYE_DATA_PATH, transform=TRANSFORM)\n",
    "deye_data_loader = data.DataLoader(deye_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "\n",
    "\n",
    "dataloaders = {'test_kaggle': test_data_loader, 'test_wa': wa_data_loader, 'test_deye': deye_data_loader}\n",
    "dataset_sizes = {'test_kaggle': len(test_data),'test_wa':len(wa_data), 'test_deye':len(deye_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_kaggle': 44836, 'test_wa': 45, 'test_deye': 38}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cnt = 6\n",
    "\n",
    "def my_plot(truth, pred):\n",
    "    auc = roc_auc_score(truth, pred)\n",
    "    print('AUC: %.4f' % auc)\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.xlabel('Specificity / True Negative Rate')\n",
    "    plt.ylabel('Sensitivity / Recall')\n",
    "    plt.savefig(str(cnt) + \".jpg\")\n",
    "    cnt += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion, optimizer, scheduler):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(1):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['test_kaggle', 'test_wa', 'test_deye']:\n",
    "        # for phase in ['test_wa', 'test_deye']:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "            print('Evaluating {}'.format(phase))\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            epoch_pred = []\n",
    "            epoch_truth = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            batch = 0\n",
    "            # batch_start = time.time()\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                batch += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                epoch_truth.extend(labels.cpu().data)\n",
    "                epoch_pred.extend(preds.cpu())\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_f1 = f1_score(epoch_truth, epoch_pred, average='weighted')\n",
    "                          \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_f1))\n",
    "            print(classification_report(epoch_truth, epoch_pred))\n",
    "            print(confusion_matrix(epoch_truth, epoch_pred))\n",
    "            print(roc_auc_score(epoch_truth, epoch_pred))\n",
    "            # my_plot(epoch_truth, epoch_pred)\n",
    "            \n",
    "        print('-'*5)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test_wa\n",
      "test_wa Loss: 9.2468 Acc: 0.1111 F1: 0.2000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.11      0.20        45\n",
      "\n",
      "   micro avg       0.11      0.11      0.11        45\n",
      "   macro avg       0.50      0.06      0.10        45\n",
      "weighted avg       1.00      0.11      0.20        45\n",
      "\n",
      "[[ 0  0]\n",
      " [40  5]]\n",
      "Evaluating test_deye\n",
      "test_deye Loss: 1.3113 Acc: 0.4474 F1: 0.4415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.33      0.36        18\n",
      "           1       0.48      0.55      0.51        20\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        38\n",
      "   macro avg       0.44      0.44      0.44        38\n",
      "weighted avg       0.44      0.45      0.44        38\n",
      "\n",
      "[[ 6 12]\n",
      " [ 9 11]]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "eval_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-316a019612b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nova_ins.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
