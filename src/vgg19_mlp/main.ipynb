{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_module():\n",
    "    mlp = nn.Sequential(\n",
    "        nn.Linear(512 * 7 * 7, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(4096, 2)\n",
    "    )\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, traindir, devdir):\n",
    "    # 4780\n",
    "    # Mean for channels 0, 1, 2: 107.27337846\n",
    "    # Std for channels 0, 1, 2: 78.4173511375\n",
    "    mean = 116.987954934 / 256; std = 71.5262653842 / 256\n",
    "    trainLoader = DataLoader(\n",
    "        datasets.ImageFolder(traindir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=False\n",
    "    )\n",
    "    # 199\n",
    "    # Mean for channels 0, 1, 2: 130.181926013\n",
    "    # Std for channels 0, 1, 2: 62.5028782841\n",
    "    mean, std = 130.935603595 / 256, 60.4500025546 / 256\n",
    "    devLoader = DataLoader(\n",
    "        datasets.ImageFolder(devdir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=False\n",
    "    )\n",
    "    print(\"create DataLoader successfully!\")\n",
    "    \n",
    "    net.train()\n",
    "    wallclock = 1\n",
    "    for ep in range(EPOCHS):\n",
    "        running_loss = 0\n",
    "        for i, data_train in enumerate(trainLoader):\n",
    "            x, y = data_train\n",
    "            x, y = Variable(x), Variable(y)\n",
    "            if torch.cuda.is_available():\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            y_p = net.forward(x)\n",
    "            loss = criterion(y_p, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.data[0]\n",
    "            if (i + 1) % 10 == 0:\n",
    "                running_loss /= 10\n",
    "                print(\"epoch: {0}, step: {1}, training loss: {2}\".format(ep + 1, i + 1, running_loss))\n",
    "                writer.add_scalar('training loss', running_loss, wallclock)\n",
    "                if (wallclock + 1) % 10 == 0:\n",
    "                    running_loss, acc = validate(net, criterion, devLoader)\n",
    "                    print(\"epoch: {0}, step: {1}, validation loss: {2}, validation acc: {3}\".format(ep + 1, i + 1, running_loss, acc))\n",
    "                    writer.add_scalar('validation loss', running_loss, wallclock)\n",
    "                    writer.add_scalar('validation accuracy', acc, wallclock)\n",
    "                wallclock += 1\n",
    "                running_loss = 0\n",
    "    torch.save(net.state_dict(), './vgg19Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, criterion, devLoader):\n",
    "    net.eval()\n",
    "    running_loss, bsz = 0, 0\n",
    "    correct_cnt, total_cnt = 0, 0\n",
    "    for data_dev in devLoader:\n",
    "        x, y = data_dev\n",
    "        x, y = Variable(x), Variable(y)\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        y_p = net.forward(x)\n",
    "        _, pred_label = torch.max(y_p.data, 1)\n",
    "        total_cnt += x.size(0)\n",
    "        correct_cnt += (pred_label == y.data).sum()\n",
    "        loss = criterion(y_p, y)\n",
    "        bsz += 1\n",
    "        running_loss += loss.data[0]\n",
    "    running_loss /= bsz\n",
    "    acc = correct_cnt / total_cnt\n",
    "    net.train()\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, featFile):\n",
    "    net.eval()\n",
    "    testSet = CustomDataset(featFile, None, isTrain=False)\n",
    "    testLoader = DataLoader(testSet, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    predictions = []\n",
    "    for i, data in enumerate(testLoader):\n",
    "        x = data\n",
    "        x = Variable(x)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        y_p = net.forward(x)\n",
    "        _, pred_label = torch.max(y_p.data, 1)\n",
    "        predictions.append(pred_label)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.datasets as dset\n",
    "# import torchvision.transforms as transforms\n",
    "# mytransform = transforms.Compose([transforms.ToTensor()])\n",
    "# cifarSet_train = dset.CIFAR10(root = \"../../cifar/train/\", train=True, download=True, transform=mytransform)\n",
    "# cifarLoader_train = DataLoader(cifarSet_train, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "# cifarSet_dev = dset.CIFAR10(root = \"../../cifar/dev/\", train=False, download=True, transform=mytransform)\n",
    "# cifarLoader_dev = DataLoader(cifarSet_dev, batch_size=10, shuffle=True, num_workers=2)\n",
    "# for data_dev in cifarLoader_dev:\n",
    "#     x, y = data_dev\n",
    "#     print(x.size(), y.size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "create DataLoader successfully!\n",
      "epoch: 1, step: 10, training loss: 3.2094411075115206\n",
      "epoch: 1, step: 20, training loss: 0.3735795602202415\n",
      "epoch: 1, step: 30, training loss: 0.33851838707923887\n",
      "epoch: 1, step: 40, training loss: 0.3818897992372513\n",
      "epoch: 1, step: 50, training loss: 0.318669793009758\n",
      "epoch: 1, step: 60, training loss: 0.4683547854423523\n",
      "epoch: 1, step: 70, training loss: 0.3986029475927353\n",
      "epoch: 1, step: 80, training loss: 0.2963207110762596\n",
      "epoch: 1, step: 90, training loss: 0.3812494814395905\n",
      "epoch: 1, step: 90, validation loss: 0.3636541409151895, validation acc: 0.8793969849246231\n",
      "epoch: 1, step: 100, training loss: 0.3686065927147865\n",
      "epoch: 1, step: 110, training loss: 0.38366655111312864\n",
      "epoch: 1, step: 120, training loss: 0.3685834765434265\n",
      "epoch: 1, step: 130, training loss: 0.33498388826847075\n",
      "epoch: 1, step: 140, training loss: 0.3881186380982399\n",
      "epoch: 1, step: 150, training loss: 0.38809225261211394\n",
      "epoch: 2, step: 10, training loss: 0.3031520187854767\n",
      "epoch: 2, step: 20, training loss: 0.3460651606321335\n",
      "epoch: 2, step: 30, training loss: 0.3762272983789444\n",
      "epoch: 2, step: 40, training loss: 0.30714054256677625\n",
      "epoch: 2, step: 40, validation loss: 0.5340225909437452, validation acc: 0.7688442211055276\n",
      "epoch: 2, step: 50, training loss: 0.30081009268760683\n",
      "epoch: 2, step: 60, training loss: 0.31715981662273407\n",
      "epoch: 2, step: 70, training loss: 0.322649684548378\n",
      "epoch: 2, step: 80, training loss: 0.3468070596456528\n",
      "epoch: 2, step: 90, training loss: 0.36379114985466005\n",
      "epoch: 2, step: 100, training loss: 0.24979792833328246\n",
      "epoch: 2, step: 110, training loss: 0.2359638914465904\n",
      "epoch: 2, step: 120, training loss: 0.34229476302862166\n",
      "epoch: 2, step: 130, training loss: 0.3430274039506912\n",
      "epoch: 2, step: 140, training loss: 0.3132624298334122\n",
      "epoch: 2, step: 140, validation loss: 0.4013290660721915, validation acc: 0.8040201005025126\n",
      "epoch: 2, step: 150, training loss: 0.3235413357615471\n",
      "epoch: 3, step: 10, training loss: 0.27342905700206754\n",
      "epoch: 3, step: 20, training loss: 0.27053868025541306\n",
      "epoch: 3, step: 30, training loss: 0.25136078111827376\n",
      "epoch: 3, step: 40, training loss: 0.3121914118528366\n",
      "epoch: 3, step: 50, training loss: 0.3217308759689331\n",
      "epoch: 3, step: 60, training loss: 0.3357356309890747\n",
      "epoch: 3, step: 70, training loss: 0.3052107855677605\n",
      "epoch: 3, step: 80, training loss: 0.27411753982305526\n",
      "epoch: 3, step: 90, training loss: 0.24588133469223977\n",
      "epoch: 3, step: 90, validation loss: 0.4724739832537515, validation acc: 0.7939698492462312\n",
      "epoch: 3, step: 100, training loss: 0.24895429909229277\n",
      "epoch: 3, step: 110, training loss: 0.2730573520064354\n",
      "epoch: 3, step: 120, training loss: 0.296782985329628\n",
      "epoch: 3, step: 130, training loss: 0.25644540339708327\n",
      "epoch: 3, step: 140, training loss: 0.2772197753190994\n",
      "epoch: 3, step: 150, training loss: 0.3174466982483864\n",
      "epoch: 4, step: 10, training loss: 0.28880188018083575\n",
      "epoch: 4, step: 20, training loss: 0.20096057429909706\n",
      "epoch: 4, step: 30, training loss: 0.32964870482683184\n",
      "epoch: 4, step: 40, training loss: 0.3113038197159767\n",
      "epoch: 4, step: 40, validation loss: 0.17620985103505, validation acc: 0.9447236180904522\n",
      "epoch: 4, step: 50, training loss: 0.2516821131110191\n",
      "epoch: 4, step: 60, training loss: 0.254905004799366\n",
      "epoch: 4, step: 70, training loss: 0.23425324484705926\n",
      "epoch: 4, step: 80, training loss: 0.21548022627830504\n",
      "epoch: 4, step: 90, training loss: 0.2749579682946205\n",
      "epoch: 4, step: 100, training loss: 0.3050453409552574\n",
      "epoch: 4, step: 110, training loss: 0.2884192124009132\n",
      "epoch: 4, step: 120, training loss: 0.2658913999795914\n",
      "epoch: 4, step: 130, training loss: 0.272648398578167\n",
      "epoch: 4, step: 140, training loss: 0.2972013294696808\n",
      "epoch: 4, step: 140, validation loss: 0.2470376268029213, validation acc: 0.8944723618090452\n",
      "epoch: 4, step: 150, training loss: 0.292129560559988\n",
      "epoch: 5, step: 10, training loss: 0.2389108270406723\n",
      "epoch: 5, step: 20, training loss: 0.22414211183786392\n",
      "epoch: 5, step: 30, training loss: 0.24029297530651092\n",
      "epoch: 5, step: 40, training loss: 0.22157283648848533\n",
      "epoch: 5, step: 50, training loss: 0.36705167293548585\n",
      "epoch: 5, step: 60, training loss: 0.28955247551202773\n",
      "epoch: 5, step: 70, training loss: 0.27391210943460464\n",
      "epoch: 5, step: 80, training loss: 0.18550608977675437\n",
      "epoch: 5, step: 90, training loss: 0.2563045918941498\n",
      "epoch: 5, step: 90, validation loss: 0.2443574901138033, validation acc: 0.8894472361809045\n",
      "epoch: 5, step: 100, training loss: 0.292581906914711\n",
      "epoch: 5, step: 110, training loss: 0.26195819973945617\n",
      "epoch: 5, step: 120, training loss: 0.29736160188913346\n",
      "epoch: 5, step: 130, training loss: 0.28100156486034394\n",
      "epoch: 5, step: 140, training loss: 0.25886387526988985\n",
      "epoch: 5, step: 150, training loss: 0.21837367489933968\n",
      "epoch: 6, step: 10, training loss: 0.24411330372095108\n",
      "epoch: 6, step: 20, training loss: 0.23811421245336534\n",
      "epoch: 6, step: 30, training loss: 0.1837022826075554\n",
      "epoch: 6, step: 40, training loss: 0.1974020816385746\n",
      "epoch: 6, step: 40, validation loss: 0.1764895138995988, validation acc: 0.9195979899497487\n",
      "epoch: 6, step: 50, training loss: 0.22027122601866722\n",
      "epoch: 6, step: 60, training loss: 0.25945691019296646\n",
      "epoch: 6, step: 70, training loss: 0.2608816049993038\n",
      "epoch: 6, step: 80, training loss: 0.2731762990355492\n",
      "epoch: 6, step: 90, training loss: 0.26418499648571014\n",
      "epoch: 6, step: 100, training loss: 0.2352489113807678\n",
      "epoch: 6, step: 110, training loss: 0.25358143299818037\n",
      "epoch: 6, step: 120, training loss: 0.19732121452689172\n",
      "epoch: 6, step: 130, training loss: 0.261459481716156\n",
      "epoch: 6, step: 140, training loss: 0.21675702929496765\n",
      "epoch: 6, step: 140, validation loss: 0.20674834932599748, validation acc: 0.9547738693467337\n",
      "epoch: 6, step: 150, training loss: 0.2419423684477806\n",
      "epoch: 7, step: 10, training loss: 0.23916080370545387\n",
      "epoch: 7, step: 20, training loss: 0.2680511802434921\n",
      "epoch: 7, step: 30, training loss: 0.17167747020721436\n",
      "epoch: 7, step: 40, training loss: 0.21256382428109646\n",
      "epoch: 7, step: 50, training loss: 0.1866818770766258\n",
      "epoch: 7, step: 60, training loss: 0.20440971329808236\n",
      "epoch: 7, step: 70, training loss: 0.2347170926630497\n",
      "epoch: 7, step: 80, training loss: 0.22954158931970597\n",
      "epoch: 7, step: 90, training loss: 0.22745088934898378\n",
      "epoch: 7, step: 90, validation loss: 0.2711766149316515, validation acc: 0.8542713567839196\n",
      "epoch: 7, step: 100, training loss: 0.21386232376098632\n",
      "epoch: 7, step: 110, training loss: 0.18098848462104797\n",
      "epoch: 7, step: 120, training loss: 0.23063731342554092\n",
      "epoch: 7, step: 130, training loss: 0.24057180881500245\n",
      "epoch: 7, step: 140, training loss: 0.2489735320210457\n",
      "epoch: 7, step: 150, training loss: 0.28811854869127274\n",
      "epoch: 8, step: 10, training loss: 0.19317907392978667\n",
      "epoch: 8, step: 20, training loss: 0.18561284393072128\n",
      "epoch: 8, step: 30, training loss: 0.2308477059006691\n",
      "epoch: 8, step: 40, training loss: 0.20659131184220314\n",
      "epoch: 8, step: 40, validation loss: 0.22914662531444005, validation acc: 0.914572864321608\n",
      "epoch: 8, step: 50, training loss: 0.16213560700416565\n",
      "epoch: 8, step: 60, training loss: 0.19285956844687463\n",
      "epoch: 8, step: 70, training loss: 0.1616456236690283\n",
      "epoch: 8, step: 80, training loss: 0.20642310082912446\n",
      "epoch: 8, step: 90, training loss: 0.21239851042628288\n",
      "epoch: 8, step: 100, training loss: 0.1409111011773348\n",
      "epoch: 8, step: 110, training loss: 0.32107507809996605\n",
      "epoch: 8, step: 120, training loss: 0.2825995787978172\n",
      "epoch: 8, step: 130, training loss: 0.2575280420482159\n",
      "epoch: 8, step: 140, training loss: 0.2706190638244152\n",
      "epoch: 8, step: 140, validation loss: 0.5874890983104706, validation acc: 0.8140703517587939\n",
      "epoch: 8, step: 150, training loss: 0.2429284170269966\n",
      "epoch: 9, step: 10, training loss: 0.16620483696460725\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"using cuda\")\n",
    "\n",
    "finetune = True\n",
    "BATCH_SIZE, EPOCHS, NUM_WORKERS = 32, 12, 1\n",
    "net_vgg19 = models.vgg19(pretrained=finetune)\n",
    "for param in net_vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "net_vgg19.classifier = mlp_module()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# use vgg19\n",
    "net = net_vgg19\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    net, criterion = net.cuda(), criterion.cuda()\n",
    "optimizer = optim.Adam(net.classifier.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "train(net, criterion, optimizer, 'Training', 'TestImages')\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()\n",
    "print(\"training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"updateVGGmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
